name: Daily EDINET Extract

on:
  schedule:
    - cron: '0 15 * * *'  # 毎日日本時間 0:00 (UTC 15:00)
  workflow_dispatch:
    inputs:
      start_date:
        description: 'Start Date (YYYY-MM-DD)'
        required: true
        default: '2024-06-01'
      end_date:
        description: 'End Date (YYYY-MM-DD)'
        required: true
        default: '2024-06-30'

jobs:
  discovery:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      start_date: ${{ steps.set-dates.outputs.start_date }}
      end_date: ${{ steps.set-dates.outputs.end_date }}
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      - name: Lint and Format with Ruff
        run: |
          ruff format .
          ruff check --fix .
      - name: Prep and Cache Data Lakehouse Masters
        run: mkdir -p data/meta
      - name: Cache Data Lakehouse Masters
        uses: actions/cache@v4
        with:
          path: data/meta
          key: ${{ runner.os }}-masters-${{ hashFiles('**/requirements.txt') }}-${{ github.run_id }}
          restore-keys: |
            ${{ runner.os }}-masters-
      - id: set-dates
        run: |
          START_DATE=$(date -d "30 days ago" +%Y-%m-%d)
          END_DATE=$(date +%Y-%m-%d)
          [[ -n "${{ github.event.inputs.start_date }}" ]] && START_DATE="${{ github.event.inputs.start_date }}"
          [[ -n "${{ github.event.inputs.end_date }}" ]] && END_DATE="${{ github.event.inputs.end_date }}"
          echo "start_date=$START_DATE" >> $GITHUB_OUTPUT
          echo "end_date=$END_DATE" >> $GITHUB_OUTPUT
      - id: set-matrix
        env:
          EDINET_API_KEY: ${{ secrets.EDINET_API_KEY }}
          HF_REPO: ${{ vars.HF_REPO }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          EXTRACT_START: ${{ steps.set-dates.outputs.start_date }}
          EXTRACT_END: ${{ steps.set-dates.outputs.end_date }}
          DISABLE_PANDERA_IMPORT_WARNING: True
        run: |
          RAW_JSON=$(python main.py --list-only --start "${{ steps.set-dates.outputs.start_date }}" --end "${{ steps.set-dates.outputs.end_date }}" | grep "^JSON_MATRIX_DATA:" | cut -d':' -f2-)
          DATA=${RAW_JSON:-"[]"}
          
          echo "$DATA" | python -c "
          import json, math, sys
          try:
              data_raw = sys.stdin.read()
              data = json.loads(data_raw)
              if not data:
                  print('matrix=[]')
                  sys.exit(0)
              
              # 【修正】全書類（824件など）を対象にする。
              # 以前は 'その他' を除外していたため56件の漏れが発生していた。
              filtered_data = data
              
              if not filtered_data:
                  sys.stderr.write('処理対象の書類が0件でした。\n')
                  print('matrix=[]')
                  sys.exit(0)
              
              sec_map = {}
              for item in filtered_data:
                  s = item.get('sector', 'その他')
                  sec_map.setdefault(s, []).append(item['id'])
              
              # 書き込み競合回避のため、業種単位でバケットに分ける
              sorted_sectors = sorted(sec_map.keys(), key=lambda x: len(sec_map[x]), reverse=True)
              # ARG_MAX 回避のため、ジョブの分割数を増やし、1つの引数が長くなりすぎないように調整
              n_jobs = 20
              buckets = [[] for _ in range(n_jobs)]
              bucket_counts = [0] * n_jobs
              res_meta = [{} for _ in range(n_jobs)]
              
              for s in sorted_sectors:
                  idx = bucket_counts.index(min(bucket_counts))
                  buckets[idx].extend(sec_map[s])
                  bucket_counts[idx] += len(sec_map[s])
                  if 'sectors' not in res_meta[idx]: res_meta[idx]['sectors'] = []
                  res_meta[idx]['sectors'].append(s)
              
              res = []
              for i, b in enumerate(buckets):
                  if not b: continue
                  sectors_str = ', '.join(res_meta[i]['sectors'])
                  display_sectors = (sectors_str[:50] + '...') if len(sectors_str) > 50 else sectors_str
                  
                  res.append({
                      'ids': ','.join(b),
                      'idx': i,
                      'count': len(b),
                      'sectors': display_sectors
                  })
              
              sys.stderr.write(f'対象期間内の総処理対象書類数: {len(filtered_data)} 件\n')
              print(f'matrix={json.dumps(res)}')
          except Exception as e:
              import traceback
              sys.stderr.write(f'Error: {traceback.format_exc()}\n')
              print('matrix=[]')
          " >> $GITHUB_OUTPUT

  extract:
    needs: discovery
    if: needs.discovery.outputs.matrix != '[]' && needs.discovery.outputs.matrix != ''
    runs-on: ubuntu-latest
    permissions:
      contents: write
    strategy:
      fail-fast: false
      matrix:
        chunk: ${{ fromJson(needs.discovery.outputs.matrix) }}
    timeout-minutes: 360
    name: extract (${{ matrix.chunk.sectors }})
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      - name: Lint and Format with Ruff
        run: |
          ruff format .
          ruff check --fix .
      - name: Prep and Cache Data Lakehouse Masters
        run: mkdir -p data/meta
      - name: Cache Data Lakehouse Masters
        uses: actions/cache@v4
        with:
          path: data/meta
          key: ${{ runner.os }}-masters-${{ hashFiles('**/requirements.txt') }}-${{ github.run_id }}
          restore-keys: |
            ${{ runner.os }}-masters-
      - name: Run Extraction
        env:
          EDINET_API_KEY: ${{ secrets.EDINET_API_KEY }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_REPO: ${{ vars.HF_REPO }}
        run: |
          # main.py の引数名は --id_list
          python main.py --start "${{ needs.discovery.outputs.start_date }}" --end "${{ needs.discovery.outputs.end_date }}" --id_list "${{ matrix.chunk.ids }}"
