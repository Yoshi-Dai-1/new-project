name: Daily EDINET Extract

on:
  schedule:
    - cron: '0 15 * * *'  # 毎日日本時間 0:00 (UTC 15:00)
  workflow_dispatch:
    inputs:
      start_date:
        description: 'Start Date (YYYY-MM-DD)'
        required: true
        default: '2024-06-01'
      end_date:
        description: 'End Date (YYYY-MM-DD)'
        required: true
        default: '2024-06-30'

jobs:
  discovery:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - id: set-matrix
        env:
          EDINET_API_KEY: ${{ secrets.EDINET_API_KEY }}
          EXTRACT_START: ${{ github.event.inputs.start_date || '2024-06-01' }}
          EXTRACT_END: ${{ github.event.inputs.end_date || '2024-06-30' }}
          DISABLE_PANDERA_IMPORT_WARNING: True
        run: |
          # 警告や進捗バーが stdout に混ざっても、最後の1行（JSON）だけを抽出して確実に取得
          DATA=$(python main.py --list-only --start "$EXTRACT_START" --end "$EXTRACT_END" | tail -n 1)
          # 業種（DBファイル）の衝突を防ぎつつ、件数を均等にするロジック
          MATRIX=$(python -c "
          import json, math, sys
          try:
              data = json.loads(sys.argv[1])
              if not data: print('[]'); exit()
              
              # 業種ごとのIDリストを作成
              sec_map = {}
              for item in data:
                  s = item['sector']
                  if s not in sec_map: sec_map[s] = []
                  sec_map[s].append(item['id'])
              
              # 業種を件数が多い順にソート（大きいものから箱に入れる）
              sorted_sectors = sorted(sec_map.keys(), key=lambda x: len(sec_map[x]), reverse=True)
              
              n = 4
              buckets = [[] for _ in range(n)]
              bucket_counts = [0] * n
              
              for s in sorted_sectors:
                  # 最も空いているバケットに入れる (Greedy)
                  idx = bucket_counts.index(min(bucket_counts))
                  buckets[idx].extend(sec_map[s])
                  bucket_counts[idx] += len(sec_map[s])
              
              res = []
              for i, b in enumerate(buckets):
                  if b: res.append({'ids': ','.join(b), 'idx': i, 'count': len(b)})
              print(json.dumps(res))
          except Exception as e:
              print(json.dumps([]))
          " "$DATA")
          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT

  extract:
    needs: discovery
    if: needs.discovery.outputs.matrix != '[]' && needs.discovery.outputs.matrix != ''
    runs-on: ubuntu-latest
    permissions:
      contents: write
    strategy:
      fail-fast: false
      matrix:
        chunk: ${{ fromJson(needs.discovery.outputs.matrix) }}
    timeout-minutes: 360
    
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run Extraction
        env:
          EDINET_API_KEY: ${{ secrets.EDINET_API_KEY }}
          EXTRACT_START: ${{ github.event.inputs.start_date || '2024-06-01' }}
          EXTRACT_END: ${{ github.event.inputs.end_date || '2024-06-30' }}
        run: python main.py --start "$EXTRACT_START" --end "$EXTRACT_END" --id-list "${{ matrix.chunk.ids }}"

      - name: Commit and Push DB
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git config --local core.quotepath false
          
          MAX_RETRIES=10
          RETRY_COUNT=0
          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            # 解析済みファイルを一時避難させてプル(rebase)を実行
            if git pull --rebase --autostash origin main; then
              if [ "$(find ./data -name "*.db" | wc -l)" -gt 0 ]; then
                git add "./data/**/*.db"
                if git diff --staged --quiet; then
                  echo "No changes to commit"
                  break
                fi
                git commit -m "chore: Extract chunk ${{ matrix.chunk.idx }} (${{ matrix.chunk.count }} docs)"
                if git push origin main; then
                  echo "Push successful"
                  break
                fi
              else
                echo "No files to commit"
                break
              fi
            fi
            RETRY_COUNT=$((RETRY_COUNT+1))
            echo "Retry $RETRY_COUNT/$MAX_RETRIES..."
            sleep $((RANDOM % 20 + 10))
          done
